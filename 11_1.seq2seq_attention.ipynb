{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import re, os, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现步骤\n",
    "## 1、预处理\n",
    "## 2、搭建模型\n",
    "### 2.1、encoder\n",
    "### 2.2、attention\n",
    "### 2.3、decoder\n",
    "### 2.5、loss & optimizer\n",
    "### 2.6、train\n",
    "## 3、evaluation搭建模型\n",
    "### 3.1、given sentence，return translated results\n",
    "### 3.2、visualize results（attention）\n",
    "\n",
    "* `https://tensorflow.google.cn/tutorials/text/nmt_with_attention?hl=zh-cn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2646016/2638744 [==============================] - 4s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# 下载文件\n",
    "#  http://www.manythings.org/anki/ 这个数据集中有很多种语言可供选择。我们将使用英语 - 西班牙语数据集。\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 将 unicode 文件转换为 ascii\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) \n",
    "                   #NFD是一种方法，如果unicode由多个ascii码组成，就把它拆开\n",
    "                   #Mn是西班牙语中的重音，下面语句就是去除重音\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # 在单词与跟在其后的标点符号之间插入一个空格\n",
    "    # 例如： \"he is a boy.\" => \"he is a boy .\"\n",
    "    # 参考：https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)#前后加空格\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)#空格去重\n",
    "\n",
    "    # 除了 (a-z, A-Z, \".\", \"?\", \"!\", \",\")，将所有字符替换为空格\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # 给句子加上开始和结束标记\n",
    "    # 以便模型知道何时开始和结束预测\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "<start> ¿ puedo tomar prestado este libro ? <end>\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence))#.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "def parse_data(filename):\n",
    "    lines = open(filename, encoding='UTF-8').read().strip().split('\\n')\n",
    "    sentence_pairs = [line.split('\\t') for line in lines]\n",
    "    preprocess_sentence_pairs = [(preprocess_sentence(en), preprocess_sentence(sp)) for en, sp in sentence_pairs]\n",
    "    \n",
    "    return zip(*preprocess_sentence_pairs)#解包和zip\n",
    "en_dataset, sp_dataset = parse_data(path_to_file)\n",
    "print(en_dataset[-1])\n",
    "print(sp_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2) (3, 4) (5, 6)\n",
      "((1, 2),) ((3, 4),) ((5, 6),)\n",
      "(1, 3, 5) (2, 4, 6)\n"
     ]
    }
   ],
   "source": [
    "# 测试 解包和zip\n",
    "a = [(1,2),(3,4),(5,6)]\n",
    "print(*a)#解包就是把每个元组解开\n",
    "# c,d,e = zip(a)\n",
    "# print(c, d, e)\n",
    "c, d  = zip(*a)解包和zip后效果\n",
    "print(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 11\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# 转换成id\n",
    "# https://www.cnblogs.com/xianhan/p/10834712.html\n",
    "def tokenizer(lang):\n",
    "    '''将文本转化为id，并同时返回转化器'''\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "            num_words=None,#限制词表数量，整数，none代表不限制\n",
    "            filters = '',#过滤掉的词语\n",
    "            split = ' '#按照空格分割              \n",
    "    )#实例化\n",
    "    lang_tokenizer.fit_on_texts(lang)#传入数据\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)#调用方法转换为seq\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor,padding = 'post')#做padding\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[:30000])#使用部分样本，测试\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[:30000])\n",
    "\n",
    "def max_length(tensor):\n",
    "    '''获取最长的样本长度'''\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_input = max_length(input_tensor)\n",
    "max_length_output = max_length(output_tensor)\n",
    "print(max_length_input, max_length_output)\n",
    "print(len(input_tensor[2]))#padding之后所有样本长度相等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 6000, 24000, 6000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(\n",
    "            input_tensor , output_tensor, test_size=0.2)\n",
    "\n",
    "len(input_train), len(input_eval), len(output_train), len(output_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --> <start>\n",
      "4 --> tom\n",
      "7 --> es\n",
      "15 --> un\n",
      "117 --> buen\n",
      "1261 --> cantante\n",
      "3 --> .\n",
      "2 --> <end>\n",
      "\n",
      "1 --> <start>\n",
      "5 --> tom\n",
      "8 --> is\n",
      "9 --> a\n",
      "69 --> good\n",
      "1065 --> singer\n",
      "3 --> .\n",
      "2 --> <end>\n"
     ]
    }
   ],
   "source": [
    "# 验证tokenizer是否正常工作\n",
    "def convert(example, tokenizer):\n",
    "    for t in example:\n",
    "        if t !=0:\n",
    "            print('%d --> %s' %(t, tokenizer.index_word[t]))\n",
    "\n",
    "convert(input_train[0], input_tokenizer)\n",
    "print()\n",
    "convert(output_train[0], output_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle = False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
    "    if shuffle:\n",
    "        detaset = dataset.shuffle(len(input_tensor))\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size,drop_remainder = True)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "train_dataset = make_dataset(input_train, output_train, batch_size, epochs, shuffle=True)\n",
    "eval_dataset = make_dataset(input_eval, output_eval, batch_size, 1) \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "(64, 11)\n",
      "tf.Tensor(\n",
      "[[   1    4    7 ...    0    0    0]\n",
      " [   1   52   21 ...    0    0    0]\n",
      " [   1  235  346 ...    0    0    0]\n",
      " ...\n",
      " [   1 1390   11 ...    0    0    0]\n",
      " [   1   25    8 ...    0    0    0]\n",
      " [   1 1848  129 ...    0    0    0]], shape=(64, 16), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[   1    5    8    9   69 1065    3    2    0    0    0]\n",
      " [   1    6   23    9   69  312    3    2    0    0    0]\n",
      " [   1  190  372   24    5   11    3    2    0    0    0]\n",
      " [   1   21   91    8 2397   17    3    2    0    0    0]\n",
      " [   1    5   87   12  116   17    3    2    0    0    0]\n",
      " [   1 3200   17    3    2    0    0    0    0    0    0]\n",
      " [   1   27    8 4179 1897    3    2    0    0    0    0]\n",
      " [   1   25    6   53   80   81    7    2    0    0    0]\n",
      " [   1   45  240   50   80   17    3    2    0    0    0]\n",
      " [   1   31   83    8  224    3    2    0    0    0    0]\n",
      " [   1    5    8  129  715    3    2    0    0    0    0]\n",
      " [   1   16  221    3    2    0    0    0    0    0    0]\n",
      " [   1   16   25  166    6    3    2    0    0    0    0]\n",
      " [   1   14  124   61  729    3    2    0    0    0    0]\n",
      " [   1   10   11   31  493    3    2    0    0    0    0]\n",
      " [   1   27    8 1370    3    2    0    0    0    0    0]\n",
      " [   1   16   30   12   22  242    3    2    0    0    0]\n",
      " [   1   14  590  144 4326    3    2    0    0    0    0]\n",
      " [   1    5   75  541    3    2    0    0    0    0    0]\n",
      " [   1    8   19   31   83    7    2    0    0    0    0]\n",
      " [   1    6   23    9  490  350    3    2    0    0    0]\n",
      " [   1   46   11  111   10    3    2    0    0    0    0]\n",
      " [   1   22    6   47   21   73    7    2    0    0    0]\n",
      " [   1    4   35   15  317 1367    3    2    0    0    0]\n",
      " [   1    5   51 3022    3    2    0    0    0    0    0]\n",
      " [   1    4   22  221    3    2    0    0    0    0    0]\n",
      " [   1 1582   13 2722    3    2    0    0    0    0    0]\n",
      " [   1    6  663    3    2    0    0    0    0    0    0]\n",
      " [   1   10   11 3310    3   18    3    2    0    0    0]\n",
      " [   1   14    8   78  138    3    2    0    0    0    0]\n",
      " [   1   82   42    6   86    7    2    0    0    0    0]\n",
      " [   1   82   87   12    6  122    7    2    0    0    0]\n",
      " [   1    4   38  272   84    3    2    0    0    0    0]\n",
      " [   1  459   10  203 1414    3    2    0    0    0    0]\n",
      " [   1    5   26  624   50    3    2    0    0    0    0]\n",
      " [   1    4  234    5   48  109    3    2    0    0    0]\n",
      " [   1 1556   17    9  354    3    2    0    0    0    0]\n",
      " [   1   88   55   20 2371    3    2    0    0    0    0]\n",
      " [   1   10   11   97   15  102    3    2    0    0    0]\n",
      " [   1    4 1389    6    3    2    0    0    0    0    0]\n",
      " [   1   28   38  479    3    2    0    0    0    0    0]\n",
      " [   1    4 1074  178    3    2    0    0    0    0    0]\n",
      " [   1    4   18   85  101    3    2    0    0    0    0]\n",
      " [   1   60   11  285    7    2    0    0    0    0    0]\n",
      " [   1    4   18   58 2492    3    2    0    0    0    0]\n",
      " [   1 1676   19 1477    3    2    0    0    0    0    0]\n",
      " [   1    5  682   45    3    2    0    0    0    0    0]\n",
      " [   1    4   18   34   85  342    3    2    0    0    0]\n",
      " [   1   67   59   81   92  928    3    2    0    0    0]\n",
      " [   1    5    8  670    6    3    2    0    0    0    0]\n",
      " [   1  237  234    3    2    0    0    0    0    0    0]\n",
      " [   1    5    8  130  722    3    2    0    0    0    0]\n",
      " [   1    5   11  310   11 1029    3    2    0    0    0]\n",
      " [   1   13  320    8  183    3    2    0    0    0    0]\n",
      " [   1   46   11  125   19  349    3    2    0    0    0]\n",
      " [   1    4  175   10  292  353    3    2    0    0    0]\n",
      " [   1  396   11   44   17    3    2    0    0    0    0]\n",
      " [   1   16   23   34 2222   50    3    2    0    0    0]\n",
      " [   1    4   18 3564   74    3    2    0    0    0    0]\n",
      " [   1    4   35   31  363    3    2    0    0    0    0]\n",
      " [   1    4   70  384   58    3    2    0    0    0    0]\n",
      " [   1    4  693   10   11    9  446    3    2    0    0]\n",
      " [   1   27   11   34    9  466    3    2    0    0    0]\n",
      " [   1   36   57   80   17    3    2    0    0    0    0]], shape=(64, 11), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "embedding_units = 256\n",
    "units = 1024\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、Encoder构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_output.shape: (64, 16, 1024)\n",
      "sample_hidden.shape: (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units, \n",
    "                                               return_sequences=True,\n",
    "                                               return_state=True,\n",
    "                                               recurrent_initializer='glorot_uniform')\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state  = self.gru(x,  initial_state = hidden)\n",
    "        return output, state\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "\n",
    "    \n",
    "encoder = Encoder(input_vocab_size, embedding_units,\n",
    "                             units, batch_size)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)\n",
    "print('sample_output.shape:', sample_output.shape)\n",
    "print('sample_hidden.shape:', sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、attention构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "# # 官网\n",
    "# class BahdanauAttention(tf.keras.layers.Layer):\n",
    "#     def __init__(self, units):\n",
    "#         super(BahdanauAttention, self).__init__()\n",
    "#         self.W1 = tf.keras.layers.Dense(units)\n",
    "#         self.W2 = tf.keras.layers.Dense(units)\n",
    "#         self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "#     def call(self, query, values):\n",
    "#         # 隐藏层的形状 == （批大小，隐藏层大小）\n",
    "#         # hidden_with_time_axis 的形状 == （批大小，1，隐藏层大小）\n",
    "#         # 这样做是为了执行加法以计算分数  \n",
    "#         hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "#         # 分数的形状 == （批大小，最大长度，1）\n",
    "#         # 我们在最后一个轴上得到 1， 因为我们把分数应用于 self.V\n",
    "#         # 在应用 self.V 之前，张量的形状是（批大小，最大长度，单位）\n",
    "#         score = self.V(tf.nn.tanh(\n",
    "#         self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "#         # 注意力权重 （attention_weights） 的形状 == （批大小，最大长度，1）\n",
    "#         attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "#         # 上下文向量 （context_vector） 求和之后的形状 == （批大小，隐藏层大小）\n",
    "#         context_vector = attention_weights * values\n",
    "#         context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "#         return context_vector, attention_weights\n",
    "\n",
    "# attention_layer = BahdanauAttention(10)\n",
    "# attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "# print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "# print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_results.shape: (batch size, units) (64, 1024)\n",
      "attention_weights.shape: (batch_size, sequence_length, 1)  (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, decoder_hidden, encoder_outputs):\n",
    "        #decoder_hidden.shape:(batch_size, units)\n",
    "        #enconder_outputs.shape:(batch_size,length,units)\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "        #before V:(batch_size, length, unirs)\n",
    "        #after V:(batch_size, length, 1)\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                self.W1(encoder_outputs) +self.W2(decoder_hidden_with_time_axis)))\n",
    "        #shape:(batch_size, length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        #cintext_vertor.shape:(batch_size, length, units)\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "        \n",
    "        #cintext_vertor.shape:(batch_size, units)        \n",
    "        context_vector = tf.reduce_sum(context_vector, axis =1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "attention_model = BahdanauAttention(units = 10)\n",
    "attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n",
    "\n",
    "print('attention_results.shape: (batch size, units)', attention_results.shape)\n",
    "print('attention_weights.shape: (batch_size, sequence_length, 1) ',attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、decoder构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab_size) (64, 4935)\n",
      "decoder_hidden.shape: (64, 1024)\n",
      "decoder_aw.shape: (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, decoding_units, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = tf.keras.layers.GRU(self.decoding_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # 用于注意力\n",
    "        self.attention = BahdanauAttention(self.decoding_units)\n",
    "\n",
    "    def call(self, x, hidden, encoding_output):\n",
    "        # 编码器输出 （enc_output） 的形状 == （批大小，最大长度，隐藏层大小）\n",
    "        context_vector, attention_weights = self.attention(hidden, encoding_output)\n",
    "        # x 在通过嵌入层前形状==（批大小，1）\n",
    "        # x 在通过嵌入层后的形状 == （批大小，1，嵌入维度）\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x 在拼接 （concatenation） 后的形状 == （批大小，1，嵌入维度 + 隐藏层大小）\n",
    "        combined_x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # 将合并后的向量传送到 GRU\n",
    "        output, state = self.gru(combined_x)\n",
    "\n",
    "        # 输出的形状 == （批大小 * 1，隐藏层大小）\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # 输出的形状 == （批大小，vocab）\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, state, attention_weights\n",
    "\n",
    "decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n",
    "\n",
    "outputs = decoder(tf.random.uniform((batch_size,1)),\n",
    "                         sample_hidden,\n",
    "                         sample_output)\n",
    "\n",
    "sample_decoder_output, decoder_hidden, decoder_aw = decoder(tf.random.uniform((batch_size, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab_size) {}'.format(sample_decoder_output.shape))\n",
    "print('decoder_hidden.shape:', decoder_hidden.shape)\n",
    "print('decoder_aw.shape:', decoder_aw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    #mask目的是去掉padding对应的损失函数\n",
    "#     equel看是否是0，是0返回True(1)\n",
    "#     logical_not取反，0返回False(0)\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask# 乘以mask后padding就不计入了\n",
    "\n",
    "    return tf.reduce_mean(loss_)#最后做平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查点（基于对象保存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n",
    "## 1、将 输入 传送至 编码器，编码器返回 编码器输出 和 编码器隐藏层状态。\n",
    "## 2、将编码器输出、编码器隐藏层状态和解码器输入（即 开始标记）传送至解码器。\n",
    "## 3、解码器返回 预测 和 解码器隐藏层状态。\n",
    "## 4、解码器隐藏层状态被传送回模型，预测被用于计算损失。\n",
    "## 5、使用 教师强制 （teacher forcing） 决定解码器的下一个输入。\n",
    "## 6、教师强制 是将 目标词 作为 下一个输入 传送至解码器的技术。\n",
    "## 7、最后一步是计算梯度，并将其应用于优化器和反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 官网内容\n",
    "# @tf.function\n",
    "# def train_step(inp, targ, encoding_hidden):\n",
    "#   loss = 0\n",
    "\n",
    "#   with tf.GradientTape() as tape:\n",
    "#     encoding_outputs, encoding_hidden = encoder(inp, encoding_hidden)\n",
    "\n",
    "#     decoding_hidden = encoding_hidden\n",
    "\n",
    "#     decoding_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "#     # 教师强制 - 将目标词作为下一个输入\n",
    "#     for t in range(1, targ.shape[1]):\n",
    "#       # 将编码器输出 （enc_output） 传送至解码器\n",
    "#       predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_output)\n",
    "\n",
    "#       loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "#       # 使用教师强制\n",
    "#       decoding_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "#   batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "#   variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "#   gradients = tape.gradient(loss, variables)\n",
    "\n",
    "#   optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "#   return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 官网内容\n",
    "@tf.function\n",
    "def train_step(inp, targ, encoding_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs, encoding_hidden = encoder(inp, encoding_hidden)\n",
    "\n",
    "        decoding_hidden = encoding_hidden\n",
    "\n",
    "#     dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    \n",
    "        for t in range(0, targ.shape[1]-1):\n",
    "          # 将编码器输出 （enc_output） 传送至解码器\n",
    "            decoding_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "            predictions, decoding_hidden, _ = decoder(\n",
    "                decoding_input, decoding_hidden, encoding_outputs)\n",
    "\n",
    "            loss += loss_function(targ[:, t+1], predictions)\n",
    "\n",
    "\n",
    "    batch_loss = loss / int(targ.shape[1])\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-92c04becefcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#遍历datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mytf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mytf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mytf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mytf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mytf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mytf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mytf/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = len(input_tensor) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    encoding_hidden = encoder.initialize_hidden_state()#初始化hidden\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):#遍历datasets\n",
    "        batch_loss = train_step(inp, targ, encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "    # 每 2 个周期（epoch），保存（检查点）一次模型\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 翻译\n",
    "### 评估函数类似于训练循环，不同之处在于在这里我们不使用 教师强制。每个时间步的解码器输入是其先前的预测、隐藏层状态和编码器输出。\n",
    "### 当模型预测 结束标记 时停止预测。\n",
    "### 存储 每个时间步的注意力权重。\n",
    "#### 请注意：对于一个输入，编码器输出仅计算一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    attention_matrix = np.zeros((max_length_output, max_length_input))\n",
    "\n",
    "    input_sentence = preprocess_sentence(input_sentence)\n",
    "\n",
    "    inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_input,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    encoding_hidden =tf.zeros((1, units))\n",
    "    encoding_output, encoding_hidden = encoder(inputs, encoding_hidden)\n",
    "\n",
    "    decoding_hidden = encoding_hidden\n",
    "        \n",
    "    decoding_input = tf.expand_dims([output_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_output):\n",
    "        predictions, dec_hidden, attention_weights = decoder(decoding_input,\n",
    "                                                             deconding_hidden,\n",
    "                                                             encoding_output)\n",
    "\n",
    "        # 存储注意力权重以便后面制图\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += output_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if output_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, input_sentence, attention_matrix\n",
    "\n",
    "        # 预测的 ID 被输送回模型\n",
    "        decoding_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, input_sentence, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意力权重制图函数\n",
    "def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention_matrix, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + input_sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence):\n",
    "    results, input_sentence, attention_matrix = evaluate(input_sentence)\n",
    "\n",
    "    print('Input: %s' % (input_sentence))\n",
    "    print('Predicted translation: {}'.format(results))\n",
    "\n",
    "    attention_matrix = attention_plot[:len(results.split(' ')), \n",
    "                                                  :len(input_sentence.split(' '))]\n",
    "    plot_attention(attention_matrix, input_sentence.split(' '), results.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 恢复最新的检查点并验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复检查点目录 （checkpoint_dir） 中最新的检查点\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 错误的翻译\n",
    "translate(u'trata de averiguarlo.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
