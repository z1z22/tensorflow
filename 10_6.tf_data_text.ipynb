{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/.keras/datasets\n"
     ]
    }
   ],
   "source": [
    "# 本教程中使用的文本文件已经进行过一些典型的预处理，主要包括删除了文档页眉和页脚，行号，章节标题。请下载这些已经被局部改动过的文件。\n",
    "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "    text_dir = keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
    "# Downloads a file from a URL if it not already in the cache.  \n",
    "parent_dir = os.path.dirname(text_dir)\n",
    "print(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将文本加载到数据集中\n",
    "#### 迭代整个文件，将整个文件加载到自己的数据集中。\n",
    "\n",
    "#### 每个样本都需要单独标记，所以请使用 tf.data.Dataset.map 来为每个样本设定标签。这将迭代数据集中的每一个样本并且返回（ example, label ）对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cowper.txt\n",
      "<TextLineDatasetV2 shapes: (), types: tf.string>\n",
      "<MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>\n",
      "1 derby.txt\n",
      "<TextLineDatasetV2 shapes: (), types: tf.string>\n",
      "<MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>\n",
      "2 butler.txt\n",
      "<TextLineDatasetV2 shapes: (), types: tf.string>\n",
      "<MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64) # tf.cast()函数的作用是执行 tensorflow 中张量数据类型转换，即将index转换为int64\n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    print(i,file_name)\n",
    "    lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
    "    print(lines_dataset)\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    print(labeled_dataset)\n",
    "    labeled_data_sets.append(labeled_dataset)\n",
    "print(type(labeled_data_sets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000\n",
    "\n",
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "  \n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Upon his lofty vessel's prow, and watch'd\", shape=(), dtype=string)\n",
      "tf.Tensor(b'To burst the phalanx, and confusion sent', shape=(), dtype=string)\n",
      "tf.Tensor(b'Check his advance, such vigour Pallas gave;', shape=(), dtype=string)\n",
      "tf.Tensor(b'her delicate hand with the gold pin of the woman\\'s brooch.\"', shape=(), dtype=string)\n",
      "tf.Tensor(b'And seek it instant. It were much unmeet', shape=(), dtype=string)\n",
      "tf.Tensor(b'Myrsinus and the Hyrminian plain between,', shape=(), dtype=string)\n",
      "tf.Tensor(b'greatest king, and had most men under him.', shape=(), dtype=string)\n",
      "tf.Tensor(b\"'Gan move toward them, and the Greeks again\", shape=(), dtype=string)\n",
      "tf.Tensor(b'Meantime arrived, to whose approach the wives', shape=(), dtype=string)\n",
      "tf.Tensor(b'To Argos, ere events shall yet have proved', shape=(), dtype=string)\n",
      "tf.Tensor(b'consulted the scales of destiny, he directs his lightning against the', shape=(), dtype=string)\n",
      "tf.Tensor(b'High-mettled horses, well survey and search', shape=(), dtype=string)\n",
      "tf.Tensor(b\"That nodded, fearful, o'er his brow; his hand\", shape=(), dtype=string)\n",
      "tf.Tensor(b'Make thy deliberate judgment nothing worth,', shape=(), dtype=string)\n",
      "tf.Tensor(b\"To noble Peleus thou, 'tis said, wast born\", shape=(), dtype=string)\n",
      "tf.Tensor(b\"The bold Laogonus, Onetor's son;\", shape=(), dtype=string)\n",
      "tf.Tensor(b'been long delayed; a man, therefore, may well be thankful if he leaves', shape=(), dtype=string)\n",
      "tf.Tensor(b'Then noble Hector thus: \"What words are these,', shape=(), dtype=string)\n",
      "tf.Tensor(b\"Writh'd with the pain the mighty King of men;\", shape=(), dtype=string)\n",
      "tf.Tensor(b'Or over land, nor any, through contempt', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for ex,i in all_labeled_data.take(20):\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将文本编码成数字\n",
    "#### 机器学习基于的是数字而非文本，所以字符串需要被转化成数字列表。 为了达到此目的，我们需要构建文本与整数的一一映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立词汇表\n",
    "\n",
    "# 首先，通过将文本标记为单独的单词集合来构建词汇表。在 TensorFlow 和 Python 中均有很多方法来达成这一目的。在本教程中:\n",
    "\n",
    "# 迭代每个样本的 numpy 值。\n",
    "# 使用 tfds.features.text.Tokenizer 来将其分割成 token。\n",
    "# 将这些 token 放入一个 Python 集合中，借此来清除重复项。\n",
    "# 获取该词汇表的大小以便于以后使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17178"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()#Tokenizer将句子转化为单词列表\n",
    "vocabulary_set = set()\n",
    "\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())# 将句子转化为单词列表\n",
    "    vocabulary_set.update(some_tokens)# 放入set中去重\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样本编码\n",
    "\n",
    "#### 通过传递 vocabulary_set 到 tfds.features.text.TokenTextEncoder 来构建一个编码器。编码器的 encode 方法传入一行文本，返回一个整数列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_datasets.core.features.text.text_encoder.TokenTextEncoder"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)# 传入set构建编码器\n",
    "type(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Upon his lofty vessel's prow, and watch'd\"\n",
      "[153, 8145, 10785, 11709, 13276, 13075, 5471, 4253, 16088]\n"
     ]
    }
   ],
   "source": [
    "# 编码器使用示例\n",
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "print(example_text)\n",
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 现在，在数据集上运行编码器（通过将编码器打包到 tf.py_function 并且传参至数据集的 map 方法的方式来运行）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "    '''使用前面构建的编码器对数据进行编码'''\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "  # py_func doesn't set the shape of the returned tensors.\n",
    "    encoded_text, label = tf.py_function(encode, \n",
    "                                       inp=[text, label], \n",
    "                                       Tout=(tf.int64, tf.int64))\n",
    "\n",
    "  # `tf.data.Datasets` work best if all components have a shape set\n",
    "  #  so set the shapes manually: \n",
    "    encoded_text.set_shape([None])# 设置数据集的形状\n",
    "    label.set_shape([])# 设置label的形状\n",
    "\n",
    "    return encoded_text, label\n",
    "\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)# 数据集的 map 方法的方式来运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将数据集分割为测试集和训练集且进行分支\n",
    "#### 使用 tf.data.Dataset.take 和 tf.data.Dataset.skip 来建立一个小一些的测试数据集和稍大一些的训练数据集。\n",
    "#### 在数据集被传入模型之前，数据集需要被分批。最典型的是，每个分支中的样本大小与格式需要一致。但是数据集中样本并不全是相同大小的（每行文本字数并不相同）。因此，使用 tf.data.Dataset.padded_batch（而不是 batch ）将样本填充到相同的大小。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes = ([None],[]))\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE,padded_shapes = ([None],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "sample_text[0], sample_labels[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于我们引入了一个新的 token 来编码（填充零），因此词汇表大小增加了一个。\n",
    "\n",
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 64)          1099456   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,178,115\n",
      "Trainable params: 1,178,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# 第一层将整数表示转换为密集矢量嵌入\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, 64))\n",
    "# 下一层是 LSTM 层，它允许模型利用上下文中理解单词含义。\n",
    "# LSTM 上的双向包装器有助于模型理解当前数据点与其之前和之后的数据点的关系。\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "# 一个或多个紧密连接的层\n",
    "# 编辑 `for` 行的列表去检测层的大小\n",
    "for units in [64, 64]:\n",
    "    model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "\n",
    "# 输出层。第一个参数是标签个数。\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "# 对于一个 softmax 分类模型来说，通常使用 sparse_categorical_crossentropy 作为其损失函数。\n",
    "# 你可以尝试其他的优化器，但是 adam 是最常用的。\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "697/697 [==============================] - 145s 209ms/step - loss: 0.5105 - accuracy: 0.7534 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "697/697 [==============================] - 130s 187ms/step - loss: 0.2961 - accuracy: 0.8696 - val_loss: 0.3697 - val_accuracy: 0.8356\n",
      "Epoch 3/3\n",
      "697/697 [==============================] - 137s 197ms/step - loss: 0.2263 - accuracy: 0.9004 - val_loss: 0.3929 - val_accuracy: 0.8396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x643df2310>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=3, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 11s 140ms/step - loss: 0.3929 - accuracy: 0.8396\n",
      "\n",
      "Eval loss: 0.39293034789682946, Eval accuracy: 0.8396000266075134\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_acc = model.evaluate(test_data)\n",
    "\n",
    "print('\\nEval loss: {}, Eval accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
