{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归模型，自定义层次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=7, micro=6, releaselevel='final', serial=0)\n",
      "numpy 1.18.1\n",
      "pandas 1.0.1\n",
      "sklearn 0.22.2\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in np, pd ,sklearn, tf, keras:\n",
    "    print(module.__name__,module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=87, shape=(10, 100), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(100)\n",
    "layer = tf.keras.layers.Dense(100,input_shape=[None,5])\n",
    "layer(tf.zeros([10,5]))\n",
    "# layer可以当作函数调用输入是None*5的矩阵None是不定值，输出是100，所以输出是None值*100的矩阵，None=10\n",
    "# shape=(10, 100), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_5/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[-0.0775263 ,  0.01405533,  0.03453822,  0.04415034, -0.10577969,\n",
       "          0.19515996,  0.14099576,  0.23275267,  0.11306225, -0.14616308,\n",
       "         -0.15068445, -0.04130307,  0.20634066, -0.08791772, -0.04781081,\n",
       "         -0.19852123, -0.01171798, -0.13351235,  0.18369652, -0.09781533,\n",
       "         -0.10898316,  0.1960584 , -0.07301024,  0.02698199, -0.04146744,\n",
       "         -0.07259442,  0.05869029,  0.02663885,  0.01857479, -0.02185616,\n",
       "         -0.15174565, -0.22268693, -0.07980961,  0.1766193 , -0.20527968,\n",
       "         -0.20306996,  0.20004956, -0.005353  ,  0.02246524,  0.00453408,\n",
       "          0.06701885, -0.2292822 , -0.12181016, -0.00824779, -0.04028757,\n",
       "         -0.1426776 , -0.18519315, -0.05168667,  0.08893673,  0.04599337,\n",
       "          0.1335    , -0.07012612, -0.2230168 , -0.05762613, -0.15796626,\n",
       "          0.09637712,  0.19617198,  0.02501847,  0.22990383,  0.04837607,\n",
       "          0.0798694 ,  0.12160207, -0.15985432, -0.17932995,  0.10107984,\n",
       "          0.17669477, -0.07180262,  0.1583577 , -0.21298935,  0.22466354,\n",
       "         -0.0839929 ,  0.03564246,  0.04881729,  0.12576638, -0.1666317 ,\n",
       "          0.2187332 ,  0.17851074,  0.11432214,  0.09743075, -0.18326451,\n",
       "          0.11578099,  0.17386888,  0.05852504,  0.06046005, -0.22296157,\n",
       "         -0.08750965,  0.0931548 ,  0.06447424, -0.20704658,  0.1935205 ,\n",
       "          0.00478302, -0.1868569 ,  0.22579767, -0.09415331,  0.00795986,\n",
       "          0.1196353 ,  0.17036478, -0.06729276, -0.23532751, -0.08156818],\n",
       "        [-0.00051773, -0.18426748,  0.06033392, -0.23656733, -0.00706576,\n",
       "         -0.14098406, -0.17834762, -0.08090656,  0.08199032, -0.0770299 ,\n",
       "          0.01933502, -0.23045284,  0.11747016, -0.14134374,  0.10406695,\n",
       "         -0.16884223,  0.11644845, -0.05704474,  0.2063454 ,  0.01788114,\n",
       "         -0.1192147 ,  0.18492569, -0.00356959,  0.09520735, -0.13181937,\n",
       "          0.2122042 , -0.02509445,  0.22197492, -0.15788817,  0.23614438,\n",
       "         -0.15327796,  0.17153828, -0.16577104, -0.1508007 ,  0.21390341,\n",
       "          0.00112219,  0.20325728,  0.21734543,  0.21663536, -0.23044127,\n",
       "         -0.17638859, -0.2011543 ,  0.04495586, -0.04887134,  0.15650718,\n",
       "          0.09152718,  0.23216279, -0.05840352, -0.15030533,  0.01822804,\n",
       "         -0.09981054,  0.2202035 , -0.15421179, -0.17653033, -0.11016463,\n",
       "          0.17968597, -0.14028151,  0.05677699,  0.12387057,  0.04563083,\n",
       "         -0.00386384, -0.03139536, -0.02187748, -0.13786769, -0.16083962,\n",
       "          0.13275214,  0.09207781, -0.17053354,  0.17543767, -0.18675749,\n",
       "         -0.08218719, -0.14694758,  0.03625812, -0.04715209,  0.06039445,\n",
       "         -0.00521822, -0.2276717 , -0.10569967,  0.12416072,  0.10826059,\n",
       "          0.2147726 ,  0.16353886,  0.18475305,  0.06429003, -0.13093011,\n",
       "         -0.14592782, -0.19648772,  0.09743457,  0.0681607 , -0.14728993,\n",
       "         -0.23650715, -0.10451463,  0.16787122,  0.23568006,  0.07239364,\n",
       "          0.15284796,  0.00677395,  0.15615804, -0.07240361,  0.22292681],\n",
       "        [ 0.13809411, -0.11806772, -0.17556983,  0.15594928, -0.18699418,\n",
       "         -0.08068492, -0.11007765,  0.21561314,  0.14864017,  0.02219118,\n",
       "          0.04676904, -0.06833978, -0.1393474 , -0.20307845,  0.08532406,\n",
       "         -0.21283342,  0.05795161, -0.20273507,  0.23521037,  0.1174375 ,\n",
       "          0.04803489, -0.13951439, -0.05490427, -0.1119159 , -0.21113594,\n",
       "          0.08438785,  0.07346965,  0.10297604,  0.102226  , -0.19226319,\n",
       "          0.02527602, -0.15896654,  0.17795642,  0.17836277, -0.09331694,\n",
       "         -0.04078752, -0.03984821,  0.08451785,  0.01083669, -0.08297272,\n",
       "         -0.10710929, -0.16916376,  0.03395452, -0.21077353, -0.21349567,\n",
       "          0.0405788 , -0.2149107 , -0.06506138, -0.13655356,  0.08765359,\n",
       "          0.03594215,  0.18655796, -0.01399764,  0.05338939,  0.1328461 ,\n",
       "          0.20022391,  0.01746954, -0.01597632, -0.00035182,  0.22089775,\n",
       "         -0.21255916, -0.20405313, -0.16784519, -0.0936321 ,  0.18436424,\n",
       "         -0.04898418,  0.0240895 , -0.00959966, -0.16881299, -0.09092911,\n",
       "          0.1685528 ,  0.15682234, -0.12396455, -0.01299912,  0.23865424,\n",
       "         -0.05680521, -0.06020917,  0.079836  ,  0.02400611,  0.10643603,\n",
       "          0.02496429, -0.15128845, -0.08951676, -0.22978643,  0.13894282,\n",
       "         -0.07541996, -0.07796851,  0.10121353, -0.16460422,  0.07325338,\n",
       "          0.11538745, -0.13181886, -0.13196728, -0.15502308, -0.10683276,\n",
       "          0.08571066, -0.02029644, -0.17012513, -0.00385739,  0.1191666 ],\n",
       "        [ 0.05173351,  0.22864313, -0.2338225 , -0.06315376,  0.22650118,\n",
       "          0.10498048,  0.05048697,  0.00280525,  0.20853044,  0.06744842,\n",
       "         -0.21726894,  0.01884781,  0.23726504,  0.17147194, -0.0320726 ,\n",
       "         -0.21652924, -0.1900188 ,  0.17877118, -0.14133924,  0.14842866,\n",
       "         -0.14576834, -0.23671186, -0.07660073,  0.20021711,  0.15625606,\n",
       "          0.21440129,  0.1685385 ,  0.0677021 ,  0.1521902 ,  0.161959  ,\n",
       "          0.05568506, -0.05890501,  0.09475084,  0.19879384, -0.03361158,\n",
       "          0.10962291,  0.15589736, -0.11103165, -0.09724022, -0.01671666,\n",
       "         -0.13342299, -0.22659692, -0.10492994, -0.19075908,  0.09087776,\n",
       "          0.07734175, -0.19231248,  0.19191815,  0.2175409 ,  0.0928383 ,\n",
       "         -0.0118999 ,  0.1445236 ,  0.22226052,  0.16935714,  0.19317888,\n",
       "          0.02719374, -0.21155563, -0.05449174,  0.08786784, -0.06184749,\n",
       "          0.04644658,  0.07473586,  0.06822632, -0.15988475,  0.22869392,\n",
       "          0.00227481,  0.05488341,  0.21806099,  0.14072464,  0.07163937,\n",
       "          0.04389499,  0.18617173, -0.17034939,  0.19639282, -0.08945647,\n",
       "         -0.2045544 , -0.16211705, -0.23531218, -0.00790498,  0.00837836,\n",
       "          0.13625132, -0.21890777,  0.14996354, -0.09289913,  0.19006176,\n",
       "         -0.15685028, -0.00732216, -0.02864677,  0.02003364,  0.11286952,\n",
       "         -0.22892772,  0.17365538, -0.04307286, -0.14901066, -0.04990639,\n",
       "          0.03379859,  0.1375127 ,  0.2192318 , -0.09463809, -0.03586173],\n",
       "        [ 0.08826788, -0.03920601, -0.18807682, -0.02698018,  0.12353693,\n",
       "         -0.22356758, -0.23303404, -0.2383951 , -0.1018791 , -0.17985713,\n",
       "          0.22662137,  0.17406814,  0.19905914, -0.23706721, -0.04886557,\n",
       "         -0.057004  ,  0.09390442,  0.10703309, -0.11431052,  0.09878872,\n",
       "          0.06247161, -0.03300484,  0.16509412, -0.17676795, -0.11804903,\n",
       "         -0.07702835,  0.20165698, -0.17302938, -0.20556955,  0.11369221,\n",
       "          0.07702671,  0.01864405, -0.04500209,  0.17644466, -0.15438265,\n",
       "         -0.06451669, -0.14751917,  0.15259968, -0.01727347,  0.18803333,\n",
       "         -0.21536562, -0.18390757,  0.10862149,  0.03164344, -0.07754135,\n",
       "         -0.23033926,  0.18988805,  0.08461641,  0.15601127, -0.04387078,\n",
       "          0.16575621,  0.02612631,  0.16523747,  0.09347339,  0.15708081,\n",
       "         -0.14645214, -0.02086039, -0.17069717,  0.14830916, -0.20990899,\n",
       "          0.17571048, -0.07534763, -0.09677157, -0.14061287, -0.09377784,\n",
       "          0.15931089, -0.02087463, -0.12408372, -0.23377809,  0.19916554,\n",
       "         -0.23125713,  0.18516721, -0.12004965, -0.10368036,  0.13714953,\n",
       "         -0.2027199 ,  0.21423794, -0.0331316 , -0.05308875,  0.19483681,\n",
       "          0.20078944,  0.09158964,  0.06403436, -0.16055503,  0.03412418,\n",
       "         -0.18496409,  0.21646784,  0.01872553, -0.04824743, -0.21888503,\n",
       "         -0.23067625,  0.05743258, -0.03166637, -0.15649681, -0.15742597,\n",
       "         -0.06506872,  0.10337295,  0.03520967, -0.08788745, -0.02744262]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer的方法\n",
    "layer.variables\n",
    "# x *w +b就是全连接层dense layer做的事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_5/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[-0.0775263 ,  0.01405533,  0.03453822,  0.04415034, -0.10577969,\n",
       "          0.19515996,  0.14099576,  0.23275267,  0.11306225, -0.14616308,\n",
       "         -0.15068445, -0.04130307,  0.20634066, -0.08791772, -0.04781081,\n",
       "         -0.19852123, -0.01171798, -0.13351235,  0.18369652, -0.09781533,\n",
       "         -0.10898316,  0.1960584 , -0.07301024,  0.02698199, -0.04146744,\n",
       "         -0.07259442,  0.05869029,  0.02663885,  0.01857479, -0.02185616,\n",
       "         -0.15174565, -0.22268693, -0.07980961,  0.1766193 , -0.20527968,\n",
       "         -0.20306996,  0.20004956, -0.005353  ,  0.02246524,  0.00453408,\n",
       "          0.06701885, -0.2292822 , -0.12181016, -0.00824779, -0.04028757,\n",
       "         -0.1426776 , -0.18519315, -0.05168667,  0.08893673,  0.04599337,\n",
       "          0.1335    , -0.07012612, -0.2230168 , -0.05762613, -0.15796626,\n",
       "          0.09637712,  0.19617198,  0.02501847,  0.22990383,  0.04837607,\n",
       "          0.0798694 ,  0.12160207, -0.15985432, -0.17932995,  0.10107984,\n",
       "          0.17669477, -0.07180262,  0.1583577 , -0.21298935,  0.22466354,\n",
       "         -0.0839929 ,  0.03564246,  0.04881729,  0.12576638, -0.1666317 ,\n",
       "          0.2187332 ,  0.17851074,  0.11432214,  0.09743075, -0.18326451,\n",
       "          0.11578099,  0.17386888,  0.05852504,  0.06046005, -0.22296157,\n",
       "         -0.08750965,  0.0931548 ,  0.06447424, -0.20704658,  0.1935205 ,\n",
       "          0.00478302, -0.1868569 ,  0.22579767, -0.09415331,  0.00795986,\n",
       "          0.1196353 ,  0.17036478, -0.06729276, -0.23532751, -0.08156818],\n",
       "        [-0.00051773, -0.18426748,  0.06033392, -0.23656733, -0.00706576,\n",
       "         -0.14098406, -0.17834762, -0.08090656,  0.08199032, -0.0770299 ,\n",
       "          0.01933502, -0.23045284,  0.11747016, -0.14134374,  0.10406695,\n",
       "         -0.16884223,  0.11644845, -0.05704474,  0.2063454 ,  0.01788114,\n",
       "         -0.1192147 ,  0.18492569, -0.00356959,  0.09520735, -0.13181937,\n",
       "          0.2122042 , -0.02509445,  0.22197492, -0.15788817,  0.23614438,\n",
       "         -0.15327796,  0.17153828, -0.16577104, -0.1508007 ,  0.21390341,\n",
       "          0.00112219,  0.20325728,  0.21734543,  0.21663536, -0.23044127,\n",
       "         -0.17638859, -0.2011543 ,  0.04495586, -0.04887134,  0.15650718,\n",
       "          0.09152718,  0.23216279, -0.05840352, -0.15030533,  0.01822804,\n",
       "         -0.09981054,  0.2202035 , -0.15421179, -0.17653033, -0.11016463,\n",
       "          0.17968597, -0.14028151,  0.05677699,  0.12387057,  0.04563083,\n",
       "         -0.00386384, -0.03139536, -0.02187748, -0.13786769, -0.16083962,\n",
       "          0.13275214,  0.09207781, -0.17053354,  0.17543767, -0.18675749,\n",
       "         -0.08218719, -0.14694758,  0.03625812, -0.04715209,  0.06039445,\n",
       "         -0.00521822, -0.2276717 , -0.10569967,  0.12416072,  0.10826059,\n",
       "          0.2147726 ,  0.16353886,  0.18475305,  0.06429003, -0.13093011,\n",
       "         -0.14592782, -0.19648772,  0.09743457,  0.0681607 , -0.14728993,\n",
       "         -0.23650715, -0.10451463,  0.16787122,  0.23568006,  0.07239364,\n",
       "          0.15284796,  0.00677395,  0.15615804, -0.07240361,  0.22292681],\n",
       "        [ 0.13809411, -0.11806772, -0.17556983,  0.15594928, -0.18699418,\n",
       "         -0.08068492, -0.11007765,  0.21561314,  0.14864017,  0.02219118,\n",
       "          0.04676904, -0.06833978, -0.1393474 , -0.20307845,  0.08532406,\n",
       "         -0.21283342,  0.05795161, -0.20273507,  0.23521037,  0.1174375 ,\n",
       "          0.04803489, -0.13951439, -0.05490427, -0.1119159 , -0.21113594,\n",
       "          0.08438785,  0.07346965,  0.10297604,  0.102226  , -0.19226319,\n",
       "          0.02527602, -0.15896654,  0.17795642,  0.17836277, -0.09331694,\n",
       "         -0.04078752, -0.03984821,  0.08451785,  0.01083669, -0.08297272,\n",
       "         -0.10710929, -0.16916376,  0.03395452, -0.21077353, -0.21349567,\n",
       "          0.0405788 , -0.2149107 , -0.06506138, -0.13655356,  0.08765359,\n",
       "          0.03594215,  0.18655796, -0.01399764,  0.05338939,  0.1328461 ,\n",
       "          0.20022391,  0.01746954, -0.01597632, -0.00035182,  0.22089775,\n",
       "         -0.21255916, -0.20405313, -0.16784519, -0.0936321 ,  0.18436424,\n",
       "         -0.04898418,  0.0240895 , -0.00959966, -0.16881299, -0.09092911,\n",
       "          0.1685528 ,  0.15682234, -0.12396455, -0.01299912,  0.23865424,\n",
       "         -0.05680521, -0.06020917,  0.079836  ,  0.02400611,  0.10643603,\n",
       "          0.02496429, -0.15128845, -0.08951676, -0.22978643,  0.13894282,\n",
       "         -0.07541996, -0.07796851,  0.10121353, -0.16460422,  0.07325338,\n",
       "          0.11538745, -0.13181886, -0.13196728, -0.15502308, -0.10683276,\n",
       "          0.08571066, -0.02029644, -0.17012513, -0.00385739,  0.1191666 ],\n",
       "        [ 0.05173351,  0.22864313, -0.2338225 , -0.06315376,  0.22650118,\n",
       "          0.10498048,  0.05048697,  0.00280525,  0.20853044,  0.06744842,\n",
       "         -0.21726894,  0.01884781,  0.23726504,  0.17147194, -0.0320726 ,\n",
       "         -0.21652924, -0.1900188 ,  0.17877118, -0.14133924,  0.14842866,\n",
       "         -0.14576834, -0.23671186, -0.07660073,  0.20021711,  0.15625606,\n",
       "          0.21440129,  0.1685385 ,  0.0677021 ,  0.1521902 ,  0.161959  ,\n",
       "          0.05568506, -0.05890501,  0.09475084,  0.19879384, -0.03361158,\n",
       "          0.10962291,  0.15589736, -0.11103165, -0.09724022, -0.01671666,\n",
       "         -0.13342299, -0.22659692, -0.10492994, -0.19075908,  0.09087776,\n",
       "          0.07734175, -0.19231248,  0.19191815,  0.2175409 ,  0.0928383 ,\n",
       "         -0.0118999 ,  0.1445236 ,  0.22226052,  0.16935714,  0.19317888,\n",
       "          0.02719374, -0.21155563, -0.05449174,  0.08786784, -0.06184749,\n",
       "          0.04644658,  0.07473586,  0.06822632, -0.15988475,  0.22869392,\n",
       "          0.00227481,  0.05488341,  0.21806099,  0.14072464,  0.07163937,\n",
       "          0.04389499,  0.18617173, -0.17034939,  0.19639282, -0.08945647,\n",
       "         -0.2045544 , -0.16211705, -0.23531218, -0.00790498,  0.00837836,\n",
       "          0.13625132, -0.21890777,  0.14996354, -0.09289913,  0.19006176,\n",
       "         -0.15685028, -0.00732216, -0.02864677,  0.02003364,  0.11286952,\n",
       "         -0.22892772,  0.17365538, -0.04307286, -0.14901066, -0.04990639,\n",
       "          0.03379859,  0.1375127 ,  0.2192318 , -0.09463809, -0.03586173],\n",
       "        [ 0.08826788, -0.03920601, -0.18807682, -0.02698018,  0.12353693,\n",
       "         -0.22356758, -0.23303404, -0.2383951 , -0.1018791 , -0.17985713,\n",
       "          0.22662137,  0.17406814,  0.19905914, -0.23706721, -0.04886557,\n",
       "         -0.057004  ,  0.09390442,  0.10703309, -0.11431052,  0.09878872,\n",
       "          0.06247161, -0.03300484,  0.16509412, -0.17676795, -0.11804903,\n",
       "         -0.07702835,  0.20165698, -0.17302938, -0.20556955,  0.11369221,\n",
       "          0.07702671,  0.01864405, -0.04500209,  0.17644466, -0.15438265,\n",
       "         -0.06451669, -0.14751917,  0.15259968, -0.01727347,  0.18803333,\n",
       "         -0.21536562, -0.18390757,  0.10862149,  0.03164344, -0.07754135,\n",
       "         -0.23033926,  0.18988805,  0.08461641,  0.15601127, -0.04387078,\n",
       "          0.16575621,  0.02612631,  0.16523747,  0.09347339,  0.15708081,\n",
       "         -0.14645214, -0.02086039, -0.17069717,  0.14830916, -0.20990899,\n",
       "          0.17571048, -0.07534763, -0.09677157, -0.14061287, -0.09377784,\n",
       "          0.15931089, -0.02087463, -0.12408372, -0.23377809,  0.19916554,\n",
       "         -0.23125713,  0.18516721, -0.12004965, -0.10368036,  0.13714953,\n",
       "         -0.2027199 ,  0.21423794, -0.0331316 , -0.05308875,  0.19483681,\n",
       "          0.20078944,  0.09158964,  0.06403436, -0.16055503,  0.03412418,\n",
       "         -0.18496409,  0.21646784,  0.01872553, -0.04824743, -0.21888503,\n",
       "         -0.23067625,  0.05743258, -0.03166637, -0.15649681, -0.15742597,\n",
       "         -0.06506872,  0.10337295,  0.03520967, -0.08788745, -0.02744262]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.trainable_variables\n",
    "# 打印所有可训练的变量\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dense in module tensorflow.python.keras.layers.core object:\n",
      "\n",
      "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
      " |  Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then\n",
      " |  it is flattened prior to the initial dot product with `kernel`.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  # as first layer in a sequential model:\n",
      " |  model = Sequential()\n",
      " |  model.add(Dense(32, input_shape=(16,)))\n",
      " |  # now the model will take as input arrays of shape (*, 16)\n",
      " |  # and output arrays of shape (*, 32)\n",
      " |  \n",
      " |  # after the first layer, you don't need to specify\n",
      " |  # the size of the input anymore:\n",
      " |  model.add(Dense(32))\n",
      " |  ```\n",
      " |  \n",
      " |  Arguments:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\")..\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(inputs, self):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Actvity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
      " |          passed, it signals the losses are conditional on some of the layer's\n",
      " |          inputs, and thus they should only be run where these inputs are\n",
      " |          available. This is the case for activity regularization losses, for\n",
      " |          instance. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_metric(self, value, aggregation=None, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
      " |          it indicates that the metric tensor provided has been aggregated\n",
      " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
      " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
      " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
      " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
      " |          aggregation='mean')`.\n",
      " |        name: String metric name.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter` and\n",
      " |          `collections`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  dynamic\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this module as passed or determined in the ctor.\n",
      " |      \n",
      " |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
      " |      parent module names.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of variables owned by this module and it's submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      ```\n",
      " |      class MyModule(tf.Module):\n",
      " |        @tf.Module.with_name_scope\n",
      " |        def __call__(self, x):\n",
      " |          if not hasattr(self, 'w'):\n",
      " |            self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
      " |          return tf.matmul(x, self.w)\n",
      " |      ```\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      ```\n",
      " |      mod = MyModule()\n",
      " |      mod(tf.ones([8, 32]))\n",
      " |      # ==> <tf.Tensor: ...>\n",
      " |      mod.w\n",
      " |      # ==> <tf.Variable ...'my_module/w:0'>\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      ```\n",
      " |      a = tf.Module()\n",
      " |      b = tf.Module()\n",
      " |      c = tf.Module()\n",
      " |      a.b = b\n",
      " |      b.c = c\n",
      " |      assert list(a.submodules) == [b, c]\n",
      " |      assert list(b.submodules) == [c]\n",
      " |      assert list(c.submodules) == []\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all,x_test,y_train_all,y_test=train_test_split(housing.data, housing.target,random_state=7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all,y_train_all,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "transfer = StandardScaler()\n",
    "x_train_scaled = transfer.fit_transform(x_train)\n",
    "x_test_scaled = transfer.transform(x_test)\n",
    "x_valid_scaled = transfer.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.5417706e-05 6.7153489e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.nn.softplus: log(1+e^x)\n",
    "# 激活函数softplus相当于平滑版的relu\n",
    "# 自定义softplus激活函数,把没有参数的激活函数变成有函数\n",
    "customized_softplus = keras.layers.Lambda(lambda x : tf.nn.softplus(x))\n",
    "print(customized_softplus([-10.,-5.,0.,5.,10.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "customized_dense_layer_10 (C (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "customized_dense_layer_11 (C (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "lambda_5 (Lambda)            (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#使用自定义的dense layer\n",
    "\n",
    "class CustomizedDenseLayer(keras.layers.Layer):\n",
    "    def __init__(self,units,activation=None,**kwarge):\n",
    "        self.units = units\n",
    "        self.activation =keras.layers.Activation(activation)\n",
    "        super().__init__(**kwarge)\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        '''构建需要的参数'''\n",
    "#         x * w + b input_shape[None,a] * [a,b]-->output_shape[None,b]\n",
    "        self.kernel = self.add_weight(name ='kernel',\n",
    "                                                 shape = (input_shape[1],self.units),\n",
    "                                                initializer = 'uniform',\n",
    "                                                trainable = True)\n",
    "        self.bias = self.add_weight(name ='bias',\n",
    "                                                 shape = (self.units,),\n",
    "                                                initializer = 'zeros',\n",
    "                                                trainable = True)\n",
    "        super(CustomizedDenseLayer,self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        '''完成正向计算'''\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30,activation='relu',input_shape=x_train.shape[1:]),\n",
    "    CustomizedDenseLayer(1),\n",
    "    customized_softplus\n",
    "    #相当于\n",
    "#     keras.layers.Dense(1, activation='softplus')\n",
    "# 或者 keras.layers.Dense(1)\n",
    "#         keras.layers.Activation('softplus')\n",
    "    ])\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "             optimizer = 'nadam',# SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "11610/11610 [==============================] - 4s 364us/sample - loss: 1.3314 - val_loss: 0.6135\n",
      "Epoch 2/5\n",
      "11610/11610 [==============================] - 2s 156us/sample - loss: 0.4754 - val_loss: 0.4434\n",
      "Epoch 3/5\n",
      "11610/11610 [==============================] - 2s 157us/sample - loss: 0.4106 - val_loss: 0.4186\n",
      "Epoch 4/5\n",
      "11610/11610 [==============================] - 2s 156us/sample - loss: 0.3989 - val_loss: 0.4064\n",
      "Epoch 5/5\n",
      "11610/11610 [==============================] - 2s 158us/sample - loss: 0.3912 - val_loss: 0.4018\n"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(patience=5,min_delta=1e-2)]\n",
    "history = model.fit(x_train_scaled,y_train,epochs=5,\n",
    "                  validation_data=(x_valid_scaled,y_valid), \n",
    "                  callbacks= callbacks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=[8,5])\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0,1)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test_scaled, y_test, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "param_distribution={\n",
    "    'hidden_layers': [1,2,3,4,5,],\n",
    "    'layer_size': np.arange(1,100),\n",
    "    'learnning_rate': reciprocal(1e-4,1e-2)\n",
    "    }\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "estimator = RandomizedSearchCV(sklearn_model, #模型estimator\n",
    "                                                 param_distribution,#定义好的搜索空间\n",
    "                                                 n_iter = 10,#要搜索的参数集合数量\n",
    "#                                                  n_jobs = 1，#并行处理数据数量\n",
    "                                                  )\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=5,min_delta=1e-2)]\n",
    "estimator.fit(x_train_scaled,y_train,epochs=100,\n",
    "                  validation_data=(x_valid_scaled,y_valid), \n",
    "                  callbacks= callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reciprocal函数演示\n",
    "# from scipy.stats import reciprocal\n",
    "# # f(x) = 1/(x*log(b/a))     a<x<b\n",
    "# reciprocal.rvs(1e-4, 1e-2, size =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最佳参数\n",
    "print('最佳参数：\\n',estimator.best_params_)\n",
    "#最佳结果\n",
    "print('最佳结果:\\n',estimator.best_score_)\n",
    "#最佳估计器\n",
    "print('最佳估计器：\\n',estimator.best_estimator_)\n",
    "#交叉验证结果\n",
    "# print('交叉验证结果:\\n',estimator.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = estimator.best_estimator_.model\n",
    "model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
