{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6-7,使用spark-scala调用tensorflow2.0训练好的模型\n",
    "\n",
    "本篇文章介绍在spark中调用训练好的tensorflow模型进行预测的方法。\n",
    "\n",
    "本文内容的学习需要一定的spark和scala基础。\n",
    "\n",
    "如果使用pyspark的话会比较简单，只需要在每个excutor上用Python加载模型分别预测就可以了。\n",
    "\n",
    "但工程上为了性能考虑，通常使用的是scala版本的spark。\n",
    "\n",
    "本篇文章我们通过TensorFlow for Java 在spark中调用训练好的tensorflow模型。\n",
    "\n",
    "利用spark的分布式计算能力，从而可以让训练好的tensorflow模型在成百上千的机器上分布式并行执行模型推断。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 〇，spark-scala调用tensorflow模型概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在spark(scala)中调用tensorflow模型进行预测需要完成以下几个步骤。\n",
    "\n",
    "（1）准备protobuf模型文件\n",
    "\n",
    "（2）创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖\n",
    "\n",
    "（3）在spark(scala)项目中driver端加载tensorflow模型调试成功\n",
    "\n",
    "（4）在spark(scala)项目中通过RDD在excutor上加载tensorflow模型调试成功\n",
    "\n",
    "（5） 在spark(scala)项目中通过DataFrame在excutor上加载tensorflow模型调试成功\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，准备protobuf模型文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用tf.keras 训练一个简单的线性回归模型，并保存成protobuf文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,optimizers\n",
    "\n",
    "## 样本数量\n",
    "n = 800\n",
    "\n",
    "## 生成测试用数据集\n",
    "X = tf.random.uniform([n,2],minval=-10,maxval=10) \n",
    "w0 = tf.constant([[2.0],[-1.0]])\n",
    "b0 = tf.constant(3.0)\n",
    "\n",
    "Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @表示矩阵乘法,增加正态扰动\n",
    "\n",
    "## 建立模型\n",
    "tf.keras.backend.clear_session()\n",
    "inputs = layers.Input(shape = (2,),name =\"inputs\") #设置输入名字为inputs\n",
    "outputs = layers.Dense(1, name = \"outputs\")(inputs) #设置输出名字为outputs\n",
    "linear = models.Model(inputs = inputs,outputs = outputs)\n",
    "linear.summary()\n",
    "\n",
    "## 使用fit方法进行训练\n",
    "linear.compile(optimizer=\"rmsprop\",loss=\"mse\",metrics=[\"mae\"])\n",
    "linear.fit(X,Y,batch_size = 8,epochs = 100)  \n",
    "\n",
    "tf.print(\"w = \",linear.layers[1].kernel)\n",
    "tf.print(\"b = \",linear.layers[1].bias)\n",
    "\n",
    "## 将模型保存成pb格式文件\n",
    "export_path = \"./data/linear_model/\"\n",
    "version = \"1\"       #后续可以通过版本号进行模型版本迭代与管理\n",
    "linear.save(export_path+version, save_format=\"tf\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {export_path+version}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看模型文件相关信息\n",
    "!saved_model_cli show --dir {export_path+str(version)} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型文件信息中这些标红的部分都是后面有可能会用到的。\n",
    "\n",
    "![](./data/模型文件信息.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果使用maven管理项目，需要添加如下 jar包依赖\n",
    "\n",
    "```\n",
    "<!-- https://mvnrepository.com/artifact/org.tensorflow/tensorflow -->\n",
    "<dependency>\n",
    "    <groupId>org.tensorflow</groupId>\n",
    "    <artifactId>tensorflow</artifactId>\n",
    "    <version>1.15.0</version>\n",
    "</dependency>\n",
    "```\n",
    "\n",
    "也可以从下面网址中直接下载 org.tensorflow.tensorflow的jar包\n",
    "\n",
    "以及其依赖的org.tensorflow.libtensorflow 和 org.tensorflowlibtensorflow_jni的jar包 放到项目中。\n",
    "\n",
    "https://mvnrepository.com/artifact/org.tensorflow/tensorflow/1.15.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三， 在spark(scala)项目中driver端加载tensorflow模型调试成功"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的示范代码在jupyter notebook中进行演示，需要安装toree以支持spark(scala)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.{tensorflow=>tf}\n",
    "\n",
    "//注：load函数的第二个参数一般都是“serve”，可以从模型文件相关信息中找到\n",
    "\n",
    "val bundle = tf.SavedModelBundle \n",
    "   .load(\"/Users/liangyun/CodeFiles/eat_tensorflow2_in_30_days/data/linear_model/1\",\"serve\")\n",
    "\n",
    "//注：在java版本的tensorflow中还是类似tensorflow1.0中静态计算图的模式，需要建立Session, 指定feed的数据和fetch的结果, 然后 run.\n",
    "//注：如果有多个数据需要喂入，可以连续用用多个feed方法\n",
    "//注：输入必须是float类型\n",
    "\n",
    "val sess = bundle.session()\n",
    "val x = tf.Tensor.create(Array(Array(1.0f,2.0f),Array(2.0f,3.0f)))\n",
    "val y =  sess.runner().feed(\"serving_default_inputs:0\", x)\n",
    "         .fetch(\"StatefulPartitionedCall:0\").run().get(0)\n",
    "\n",
    "val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)\n",
    "y.copyTo(result)\n",
    "\n",
    "if(x != null) x.close()\n",
    "if(y != null) y.close()\n",
    "if(sess != null) sess.close()\n",
    "if(bundle != null) bundle.close()  \n",
    "\n",
    "result\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出如下：\n",
    "\n",
    "```\n",
    "Array(Array(3.019596), Array(3.9878292))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/TfDriver.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，在spark(scala)项目中通过RDD在excutor上加载tensorflow模型调试成功"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们通过广播机制将Driver端加载的TensorFlow模型传递到各个excutor上，并在excutor上分布式地调用模型进行推断。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.{tensorflow=>tf}\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(\"TfRDD\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "\n",
    "//在Driver端加载模型\n",
    "val bundle = tf.SavedModelBundle \n",
    "   .load(\"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1\",\"serve\")\n",
    "\n",
    "//利用广播将模型发送到excutor上\n",
    "val broads = sc.broadcast(bundle)\n",
    "\n",
    "//构造数据集\n",
    "val rdd_data = sc.makeRDD(List(Array(1.0f,2.0f),Array(3.0f,5.0f),Array(6.0f,7.0f),Array(8.0f,3.0f)))\n",
    "\n",
    "//通过mapPartitions调用模型进行批量推断\n",
    "val rdd_result = rdd_data.mapPartitions(iter => {\n",
    "    \n",
    "    val arr = iter.toArray\n",
    "    val model = broads.value\n",
    "    val sess = model.session()\n",
    "    val x = tf.Tensor.create(arr)\n",
    "    val y =  sess.runner().feed(\"serving_default_inputs:0\", x)\n",
    "             .fetch(\"StatefulPartitionedCall:0\").run().get(0)\n",
    "\n",
    "    //将预测结果拷贝到相同shape的Float类型的Array中\n",
    "    val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)\n",
    "    y.copyTo(result)\n",
    "    result.iterator\n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "rdd_result.take(5)\n",
    "bundle.close\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出如下：\n",
    "\n",
    "```\n",
    "Array(Array(3.019596), Array(3.9264367), Array(7.8607616), Array(15.974984))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/TfRDD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五， 在spark(scala)项目中通过DataFrame在excutor上加载tensorflow模型调试成功"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了可以在Spark的RDD数据上调用tensorflow模型进行分布式推断，\n",
    "\n",
    "我们也可以在DataFrame数据上调用tensorflow模型进行分布式推断。\n",
    "\n",
    "主要思路是将推断方法注册成为一个sparkSQL函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.{tensorflow=>tf}\n",
    "\n",
    "object TfDataFrame extends Serializable{\n",
    "    \n",
    "    \n",
    "    def main(args:Array[String]):Unit = {\n",
    "        \n",
    "        val spark = SparkSession\n",
    "        .builder()\n",
    "        .appName(\"TfDataFrame\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "        val sc = spark.sparkContext\n",
    "        \n",
    "        \n",
    "        import spark.implicits._\n",
    "\n",
    "        val bundle = tf.SavedModelBundle \n",
    "           .load(\"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1\",\"serve\")\n",
    "\n",
    "        val broads = sc.broadcast(bundle)\n",
    "        \n",
    "        //构造预测函数，并将其注册成sparkSQL的udf\n",
    "        val tfpredict = (features:WrappedArray[Float])  => {\n",
    "            val bund = broads.value\n",
    "            val sess = bund.session()\n",
    "            val x = tf.Tensor.create(Array(features.toArray))\n",
    "            val y =  sess.runner().feed(\"serving_default_inputs:0\", x)\n",
    "                     .fetch(\"StatefulPartitionedCall:0\").run().get(0)\n",
    "            val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)\n",
    "            y.copyTo(result)\n",
    "            val y_pred = result(0)(0)\n",
    "            y_pred\n",
    "        }\n",
    "        spark.udf.register(\"tfpredict\",tfpredict)\n",
    "        \n",
    "        //构造DataFrame数据集，将features放到一列中\n",
    "        val dfdata = sc.parallelize(List(Array(1.0f,2.0f),Array(3.0f,5.0f),Array(7.0f,8.0f))).toDF(\"features\")\n",
    "        dfdata.show \n",
    "        \n",
    "        //调用sparkSQL预测函数，增加一个新的列作为y_preds\n",
    "        val dfresult = dfdata.selectExpr(\"features\",\"tfpredict(features) as y_preds\")\n",
    "        dfresult.show \n",
    "        bundle.close\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "TfDataFrame.main(Array())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+----------+\n",
    "|  features|\n",
    "+----------+\n",
    "|[1.0, 2.0]|\n",
    "|[3.0, 5.0]|\n",
    "|[7.0, 8.0]|\n",
    "+----------+\n",
    "\n",
    "+----------+---------+\n",
    "|  features|  y_preds|\n",
    "+----------+---------+\n",
    "|[1.0, 2.0]| 3.019596|\n",
    "|[3.0, 5.0]|3.9264367|\n",
    "|[7.0, 8.0]| 8.828995|\n",
    "+----------+---------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上我们分别在spark 的RDD数据结构和DataFrame数据结构上实现了调用一个tf.keras实现的线性回归模型进行分布式模型推断。\n",
    "\n",
    "在本例基础上稍作修改则可以用spark调用训练好的各种复杂的神经网络模型进行分布式模型推断。\n",
    "\n",
    "但实际上tensorflow并不仅仅适合实现神经网络，其底层的计算图语言可以表达各种数值计算过程。\n",
    "\n",
    "利用其丰富的低阶API，我们可以在tensorflow2.0上实现任意机器学习模型，\n",
    "\n",
    "结合tf.Module提供的便捷的封装功能，我们可以将训练好的任意机器学习模型导出成模型文件并在spark上分布式调用执行。\n",
    "\n",
    "这无疑为我们的工程应用提供了巨大的想象空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('mytf': conda)",
   "language": "python",
   "name": "python37664bitmytfconda841baf2f843f47f398642a391af5d6cb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
