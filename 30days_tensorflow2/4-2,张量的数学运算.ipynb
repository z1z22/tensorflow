{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-2,张量的数学运算\n",
    "\n",
    "张量的操作主要包括张量的结构操作和张量的数学运算。\n",
    "\n",
    "张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。\n",
    "\n",
    "张量数学运算主要有：标量运算，向量运算，矩阵运算。另外我们会介绍张量运算的广播机制。\n",
    "\n",
    "本篇我们介绍张量的数学运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，标量运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。\n",
    "\n",
    "加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。\n",
    "\n",
    "标量运算符的特点是对张量实施逐元素运算。\n",
    "\n",
    "有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。\n",
    "\n",
    "许多标量运算符都在 tf.math模块下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.math Functions\n",
    "\n",
    "abs（...）：计算张量的绝对值。\n",
    "\n",
    "accumulate_n（...）：返回张量列表的按元素求和。\n",
    "\n",
    "acos（...）：计算x元素的acos。\n",
    "\n",
    "acosh（...）：计算元素x的反双曲余弦值。\n",
    "\n",
    "add（...）：按元素返回x + y。\n",
    "\n",
    "add_n（...）：将所有输入张量逐个元素相加。\n",
    "\n",
    "angle（...）：返回一个复数（或实数）张量的逐元素参数。\n",
    "\n",
    "argmax（...）：返回跨张量轴的最大值的索引。\n",
    "\n",
    "argmin（...）：返回跨张量轴的最小值的索引。\n",
    "\n",
    "asin（...）：计算元素x的三角反正弦。\n",
    "\n",
    "asinh（...）：计算元素x的反双曲正弦值。\n",
    "\n",
    "atan（...）：计算x元素方向的三角逆切线。\n",
    "\n",
    "atan2（...）：计算y / x元素方向的反正切值，并遵守参数的符号。\n",
    "\n",
    "atanh（...）：计算元素x的反双曲正切值。\n",
    "\n",
    "bessel_i0（...）：计算x元素的Bessel i0函数。\n",
    "\n",
    "bessel_i0e（...）：计算x元素的Bessel i0e函数。\n",
    "\n",
    "bessel_i1（...）：计算x元素的Bessel i1函数。\n",
    "\n",
    "bessel_i1e（...）：计算x元素的Bessel i1e函数。\n",
    "\n",
    "betainc（...）：计算正则化的不完整beta积分。\n",
    "\n",
    "bincount（...）：计算整数数组中每个值的出现次数。\n",
    "\n",
    "ceil（...）：返回不小于x的按元素最小的整数。\n",
    "\n",
    "confusion_matrix（...）：根据预测和标签计算混淆矩阵。\n",
    "\n",
    "conj（...）：返回复数的复共轭。\n",
    "\n",
    "cos（...）：计算x元素的cos。\n",
    "\n",
    "cosh（...）：计算元素x的双曲余弦值。\n",
    "\n",
    "count_nonzero（...）：计算张量维度上的非零元素数。\n",
    "\n",
    "cumprod（...）：计算张量x沿轴的累加积。\n",
    "\n",
    "cumsum（...）：计算沿轴的张量x的累积和。\n",
    "\n",
    "cumulative_logsumexp（...）：计算沿轴的张量x的累积log-sum-exp。\n",
    "\n",
    "digamma（...）：计算Psi，Lgamma的导数（的绝对值的对数\n",
    "\n",
    "divide（...）：计算x的Python样式除以y。\n",
    "\n",
    "divide_no_nan（...）：计算安全除数，如果y为零，则返回0。\n",
    "\n",
    "equal（...）：按元素返回（x == y）的真值。\n",
    "\n",
    "erf（...）：计算x元素的高斯误差函数。\n",
    "\n",
    "erfc（...）：计算元素x的互补误差函数。\n",
    "\n",
    "erfinv（...）：计算反误差函数。\n",
    "\n",
    "exp（...）：计算x元素的指数。 。\n",
    "\n",
    "expm1（...）：计算exp（x）-1个元素。\n",
    "\n",
    "floor（...）：返回不大于x的按元素最大的整数。\n",
    "\n",
    "floordiv（...）：将x / y元素相除，四舍五入到最负的整数。\n",
    "\n",
    "floormod（...）：返回元素的除法余数。当x <0 xor y <0为\n",
    "\n",
    "Greater（...）：按元素返回（x> y）的真值。\n",
    "\n",
    "Greater_equal（...）：逐元素返回（x> = y）的真值。\n",
    "\n",
    "igamma（...）：计算下正则化不完全Gamma函数P（a，x）。\n",
    "\n",
    "igammac（...）：计算上正则化的不完全Gamma函数Q（a，x）。\n",
    "\n",
    "imag（...）：返回复数（或实数）张量的虚部。\n",
    "\n",
    "in_top_k（...）：说出目标是否在前K个预测中。\n",
    "\n",
    "invert_permutation（...）：计算张量的逆排列。\n",
    "\n",
    "is_finite（...）：返回x的哪些元素是有限的。\n",
    "\n",
    "is_inf（...）：返回x的哪些元素是Inf。\n",
    "\n",
    "is_nan（...）：返回x的哪些元素为NaN。\n",
    "\n",
    "is_non_decreasing（...）：如果x为非递减值，则返回True。\n",
    "\n",
    "is_strictly_increasing（...）：如果x严格增加，则返回True。\n",
    "\n",
    "l2_normalize（...）：使用L2范数沿尺寸轴标准化。\n",
    "\n",
    "lbeta（...）：计算，沿最后一个维缩小。\n",
    "\n",
    "less（...）：按元素返回（x <y）的真值。\n",
    "\n",
    "less_equal（...）：按元素返回（x <= y）的真值。\n",
    "\n",
    "lgamma（...）：按元素计算Gamma（x）绝对值的对数。\n",
    "\n",
    "log（...）：计算x元素的自然对数。\n",
    "\n",
    "log1p（...）：按元素计算（1 + x）的自然对数。\n",
    "\n",
    "log_sigmoid（...）：计算x元素的log sigmoid。\n",
    "\n",
    "log_softmax（...）：计算日志softmax激活。\n",
    "\n",
    "logical_and（...）：按元素返回x和y的真值。\n",
    "\n",
    "logical_not（...）：返回非x元素值的真值。\n",
    "\n",
    "logical_or（...）：返回x或y元素的真值。\n",
    "\n",
    "logical_xor（...）：逻辑XOR函数。\n",
    "\n",
    "maximum（...）：按元素返回x和y的最大值（即x> y？x：y）。\n",
    "\n",
    "minimum（...）：返回元素的x和y的最小值（即x <y？x：y）。\n",
    "\n",
    "mod（...）：返回元素的除法余数。当x <0 xor y <0为\n",
    "\n",
    "multiply（...）：按元素返回x * y。\n",
    "\n",
    "multiple_no_nan（...）：计算x和y的乘积，如果y为零，即使x为NaN或无限，也将返回0。\n",
    "\n",
    "ndtri（...）：计算标准法线的分位数。\n",
    "\n",
    "negative（...）：按元素计算数值负值。\n",
    "\n",
    "nextafter（...）：直接返回x1的下一个可表示值\n",
    "\n",
    "not_equal（...）：按元素返回（x！= y）的真值。\n",
    "\n",
    "polygamma（...）：计算polygamma函数。\n",
    "\n",
    "polyval（...）：计算多项式的元素值。\n",
    "\n",
    "pow（...）：计算一个值对另一个值的幂。\n",
    "\n",
    "real（...）：返回复数（或实数）张量的实部。\n",
    "\n",
    "reciprocal（...）：计算x元素的倒数。\n",
    "\n",
    "reciprocal_no_nan（...）：在元素方面执行安全的互操作。\n",
    "\n",
    "reduce_all（...）：计算张量维度上元素的“逻辑和”。\n",
    "\n",
    "reduce_any（...）：计算张量维度上元素的“逻辑或”。\n",
    "\n",
    "reduce_euclidean_norm（...）：计算张量维度上元素的欧几里得范数。\n",
    "\n",
    "reduce_logsumexp（...）：计算log（sum（exp（横跨张量维的元素）））。\n",
    "\n",
    "reduce_max（...）：计算张量维度上的元素最大值。\n",
    "\n",
    "reduce_mean（...）：计算张量维度上元素的均值。\n",
    "\n",
    "reduce_min（...）：计算张量维度上的最小元素。\n",
    "\n",
    "reduce_prod（...）：计算张量维度上元素的乘积。\n",
    "\n",
    "reduce_std（...）：计算张量维度上元素的标准偏差。\n",
    "\n",
    "reduce_sum（...）：计算张量维度上的元素之和。\n",
    "\n",
    "reduce_variance（...）：计算张量维度上元素的方差。\n",
    "\n",
    "rint（...）：返回最接近x的按元素整数。\n",
    "\n",
    "round（...）：将张量的值四舍五入为最接近的整数（逐元素）。\n",
    "\n",
    "rsqrt（...）：计算元素x的平方根的倒数。\n",
    "\n",
    "scalar_mul（...）：将标量乘以Tensor或IndexedSlices对象。\n",
    "\n",
    "segment_max（...）：计算沿张量线段的最大值。\n",
    "\n",
    "segment_mean（...）：计算张量各段的均值。\n",
    "\n",
    "segment_min（...）：计算沿张量线段的最小值。\n",
    "\n",
    "segment_prod（...）：沿着张量的分段计算乘积。\n",
    "\n",
    "segment_sum（...）：计算张量各段的和。\n",
    "\n",
    "sigmoid（...）：计算x元素的S形。\n",
    "\n",
    "sign（...）：返回数字符号的逐元素指示。\n",
    "\n",
    "sin（...）：计算x元素正弦值。\n",
    "\n",
    "sinh（...）：计算元素x的双曲正弦值。\n",
    "\n",
    "softmax（...）：计算softmax激活。\n",
    "\n",
    "softplus（...）：计算softplus：log（exp（features）+ 1）。\n",
    "\n",
    "softsign（...）：计算softsign：功能/（abs（features）+ 1）。\n",
    "\n",
    "sqrt（...）：计算元素x的平方根。\n",
    "\n",
    "square（...）：计算元素x的平方。\n",
    "\n",
    "squared_difference（...）：按元素返回（x-y）（x-y）。\n",
    "\n",
    "subtract（...）：返回x-y元素级。\n",
    "\n",
    "tan（...）：计算x元素的tan。\n",
    "\n",
    "tanh（...）：计算元素x的双曲正切值。\n",
    "\n",
    "top_k（...）：查找最后一个维度的k个最大条目的值和索引。\n",
    "\n",
    "truediv（...）：按元素划分x / y（使用Python 3除法运算符语义）。\n",
    "\n",
    "unsorted_segment_max（...）：计算沿张量线段的最大值。\n",
    "\n",
    "unsorted_segment_mean（...）：计算沿张量线段的均值。\n",
    "\n",
    "unsorted_segment_min（...）：计算沿张量线段的最小值。\n",
    "\n",
    "unsorted_segment_prod（...）：沿着张量的分段计算乘积。\n",
    "\n",
    "unsorted_segment_sqrt_n（...）：计算张量的各部分之和除以sqrt（N）。\n",
    "\n",
    "unsorted_segment_sum（...）：计算沿张量线段的和。\n",
    "\n",
    "xdivy（...）：如果x == 0，则返回0，否则，按元素方式返回x / y。\n",
    "\n",
    "xlogy（...）：如果x == 0，则返回0；否则，按元素方式返回x * log（y）。\n",
    "\n",
    "zero_fraction（...）：返回值中零的分数。\n",
    "\n",
    "zeta（...）：计算Hurwitz zeta函数。\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1.0,2],[-3,4.0]])\n",
    "b = tf.constant([[5.0,6],[7.0,8.0]])\n",
    "a+b  #运算符重载==tf.math.add(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 6.,  8.],\n",
    "       [ 4., 12.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a-b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ -4.,  -4.],\n",
    "       [-10.,  -4.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[  5.,  12.],\n",
    "       [-21.,  32.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a/b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 0.2       ,  0.33333334],\n",
    "       [-0.42857143,  0.5       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 1.,  4.],\n",
    "       [ 9., 16.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a**(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[1.       , 1.4142135],\n",
    "       [      nan, 2.       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a%3 #mod的运算符重载，等价于m = tf.math.mod(a,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 0], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a//3  #地板除法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[ 0.,  0.],\n",
    "       [-1.,  1.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a>=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
    "array([[False,  True],\n",
    "       [False,  True]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a>=2)&(a<=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
    "array([[False,  True],\n",
    "       [False, False]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a>=2)|(a<=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
    "array([[ True,  True],\n",
    "       [ True,  True]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a==5 #tf.equal(a,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3,), dtype=bool, numpy=array([False, False, False])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[1.       , 1.4142135],\n",
    "       [      nan, 2.       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1.0,8.0])\n",
    "b = tf.constant([5.0,6.0])\n",
    "c = tf.constant([6.0,7.0])\n",
    "tf.add_n([a,b,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([12., 21.], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.print(tf.maximum(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[5 8]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.print(tf.minimum(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1 6]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，向量运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。\n",
    "许多向量运算符都以reduce开头。区别于 np.sum()等不加reduce\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "5\n",
      "9\n",
      "1\n",
      "362880\n"
     ]
    }
   ],
   "source": [
    "#向量reduce\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.reduce_sum(a))\n",
    "tf.print(tf.reduce_mean(a))\n",
    "tf.print(tf.reduce_max(a))\n",
    "tf.print(tf.reduce_min(a))\n",
    "tf.print(tf.reduce_prod(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#张量指定维度进行reduce\n",
    "b = tf.reshape(a,(3,3))\n",
    "tf.print(tf.reduce_sum(b, axis=1, keepdims=True))\n",
    "tf.print(tf.reduce_sum(b, axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[6]\n",
    " [15]\n",
    " [24]]\n",
    "[[12 15 18]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bool类型的reduce\n",
    "p = tf.constant([True,False,False])\n",
    "q = tf.constant([False,False,True])\n",
    "tf.print(tf.reduce_all(p))\n",
    "tf.print(tf.reduce_any(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用tf.foldr实现tf.reduce_sum\n",
    "s = tf.foldr(lambda a,b:a+b,tf.range(10)) \n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "45\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cum扫描累积\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.math.cumsum(a))\n",
    "tf.print(tf.math.cumprod(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[1 3 6 ... 28 36 45]\n",
    "[1 2 6 ... 5040 40320 362880]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#arg最大最小值索引\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.argmax(a))\n",
    "tf.print(tf.argmin(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "8\n",
    "0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 7 5]\n",
      "[5 2 3]\n"
     ]
    }
   ],
   "source": [
    "#tf.math.top_k可以用于对张量排序\n",
    "a = tf.constant([1,3,7,5,4,8])\n",
    "\n",
    "values,indices = tf.math.top_k(a,3)# k=3返回的是最后一个维度3个最大的数以及这三个数的索引\n",
    "tf.print(values)\n",
    "tf.print(indices)\n",
    "\n",
    "#利用tf.math.top_k可以在TensorFlow中实现KNN算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[29 5 36 45 35]\n",
      "  [13 11 27 26 42]\n",
      "  [0 51 10 12 55]\n",
      "  [48 27 3 8 42]]\n",
      "\n",
      " [[35 31 43 49 53]\n",
      "  [4 20 11 46 12]\n",
      "  [45 44 41 24 35]\n",
      "  [14 53 1 45 37]]\n",
      "\n",
      " [[32 19 59 15 57]\n",
      "  [19 43 16 34 23]\n",
      "  [26 33 59 29 46]\n",
      "  [27 11 54 47 52]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[38 38 17 18 57]\n",
      "  [10 30 32 31 48]\n",
      "  [11 35 25 50 31]\n",
      "  [27 53 45 26 58]]\n",
      "\n",
      " [[29 43 42 1 26]\n",
      "  [49 49 12 6 53]\n",
      "  [40 46 43 25 13]\n",
      "  [33 17 57 27 45]]\n",
      "\n",
      " [[34 59 14 34 28]\n",
      "  [55 57 10 54 27]\n",
      "  [18 31 39 44 16]\n",
      "  [24 15 25 55 33]]]\n",
      "[[[45 36 35]\n",
      "  [42 27 26]\n",
      "  [55 51 12]\n",
      "  [48 42 27]]\n",
      "\n",
      " [[53 49 43]\n",
      "  [46 20 12]\n",
      "  [45 44 41]\n",
      "  [53 45 37]]\n",
      "\n",
      " [[59 57 32]\n",
      "  [43 34 23]\n",
      "  [59 46 33]\n",
      "  [54 52 47]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[57 38 38]\n",
      "  [48 32 31]\n",
      "  [50 35 31]\n",
      "  [58 53 45]]\n",
      "\n",
      " [[43 42 29]\n",
      "  [53 49 49]\n",
      "  [46 43 40]\n",
      "  [57 45 33]]\n",
      "\n",
      " [[59 34 34]\n",
      "  [57 55 54]\n",
      "  [44 39 31]\n",
      "  [55 33 25]]]\n",
      "[[[3 2 4]\n",
      "  [4 2 3]\n",
      "  [4 1 3]\n",
      "  [0 4 1]]\n",
      "\n",
      " [[4 3 2]\n",
      "  [3 1 4]\n",
      "  [0 1 2]\n",
      "  [1 3 4]]\n",
      "\n",
      " [[2 4 0]\n",
      "  [1 3 4]\n",
      "  [2 4 1]\n",
      "  [2 4 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[4 0 1]\n",
      "  [4 2 3]\n",
      "  [3 1 4]\n",
      "  [4 1 2]]\n",
      "\n",
      " [[1 2 0]\n",
      "  [4 0 1]\n",
      "  [1 2 0]\n",
      "  [2 4 0]]\n",
      "\n",
      " [[1 0 3]\n",
      "  [1 0 3]\n",
      "  [3 2 1]\n",
      "  [3 4 2]]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.uniform([7,4,5],0,60, dtype = tf.int32)\n",
    "tf.print(a)\n",
    "values,indices = tf.math.top_k(a,3)# k=3返回的是最后一个维度3个最大的数以及这三个数的索引\n",
    "tf.print(values)\n",
    "tf.print(indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，矩阵运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵必须是二维的。类似tf.constant([1,2,3])这样的不是矩阵。\n",
    "\n",
    "矩阵运算包括：矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。\n",
    "\n",
    "除了一些常用的运算外，大部分和矩阵有关的运算都在tf.linalg子包中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵乘法\n",
    "a = tf.constant([[1,2],[3,4]])\n",
    "b = tf.constant([[2,0],[0,2]])\n",
    "a@b  #等价于tf.matmul(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
    "array([[2, 4],\n",
    "       [6, 8]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵转置\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.transpose(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[1., 3.],\n",
    "       [2., 4.]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.00000024 1.00000012]\n",
      " [1.50000012 -0.50000006]]\n"
     ]
    }
   ],
   "source": [
    "#矩阵逆，必须为tf.float32或tf.double类型\n",
    "a = tf.constant([[1.0,2],[3.0,4]],dtype = tf.float32)\n",
    "tf.print(tf.linalg.inv(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#矩阵求trace\n",
    "# 在线性代数中，一个n×n矩阵A的主对角线（从左上方至右下方的对角线）上各个元素的总和被称为矩阵A的迹（或迹数），一般记作tr(A)。\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.print(tf.linalg.trace(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.47722578\n"
     ]
    }
   ],
   "source": [
    "#矩阵求范数\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.print(tf.linalg.norm(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n"
     ]
    }
   ],
   "source": [
    "#矩阵行列式\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.print(tf.linalg.det(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵特征值\n",
    "tf.linalg.eigvalsh(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.8541021,  5.854102 ], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵qr分解\n",
    "a  = tf.constant([[1.0,2.0],[3.0,4.0]],dtype = tf.float32)\n",
    "q,r = tf.linalg.qr(a)\n",
    "tf.print(q)\n",
    "tf.print(r)\n",
    "tf.print(q@r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[-0.316227794 -0.948683321]\n",
    " [-0.948683321 0.316227734]]\n",
    "[[-3.1622777 -4.4271884]\n",
    " [0 -0.632455349]]\n",
    "[[1.00000012 1.99999976]\n",
    " [3 4]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵svd分解\n",
    "a  = tf.constant([[1.0,2.0],[3.0,4.0]],dtype = tf.float32)\n",
    "v,s,d = tf.linalg.svd(a)\n",
    "tf.matmul(tf.matmul(s,tf.linalg.diag(v)),d)\n",
    "\n",
    "#利用svd分解可以在TensorFlow中实现主成分分析降维\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[0.9999996, 1.9999996],\n",
    "       [2.9999998, 4.       ]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow的广播规则和numpy是一样的:\n",
    "\n",
    "* 1、如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。\n",
    "* 2、如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。\n",
    "* 3、如果两个张量在所有维度上都是相容的，它们就能使用广播。\n",
    "* 4、广播之后，每个维度的长度将取两个张量在该维度长度的较大值。\n",
    "* 5、在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。\n",
    "\n",
    "tf.broadcast_to 以显式的方式按照广播机制扩展张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1,2,3])\n",
    "b = tf.constant([[0,0,0],[1,1,1],[2,2,2]])\n",
    "b + a  #等价于 b + tf.broadcast_to(a,b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
    "array([[1, 2, 3],\n",
    "       [2, 3, 4],\n",
    "       [3, 4, 5]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.broadcast_to(a,b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
    "array([[1, 2, 3],\n",
    "       [1, 2, 3],\n",
    "       [1, 2, 3]], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算广播后计算结果的形状，静态形状，TensorShape类型参数\n",
    "tf.broadcast_static_shape(a.shape,b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorShape([3, 3])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算广播后计算结果的形状，动态形状，Tensor类型参数\n",
    "c = tf.constant([1,2,3])\n",
    "d = tf.constant([[1],[2],[3]])\n",
    "tf.broadcast_dynamic_shape(tf.shape(c),tf.shape(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#广播效果\n",
    "c+d #等价于 tf.broadcast_to(c,[3,3]) + tf.broadcast_to(d,[3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
    "array([[6.5760484, 7.8174157],\n",
    "       [6.8174157, 6.4239516]], dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"Python与算法之美\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "![image.png](./data/Python与算法之美logo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('mytf': conda)",
   "language": "python",
   "name": "python37664bitmytfconda841baf2f843f47f398642a391af5d6cb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
